# Classificador de Sentimentos de Cr√≠ticas de Filmes com RNN/LSTM e TensorFlow

![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)![Keras](https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&logo=Keras&logoColor=white)![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)

## üìñ Vis√£o Geral do Projeto

Este projeto demonstra a constru√ß√£o, treinamento e avalia√ß√£o de um modelo de **Rede Neural Recorrente (RNN)** com uma camada **LSTM Bidirecional** para a tarefa de **An√°lise de Sentimentos**. O objetivo √© classificar cr√≠ticas de filmes (em ingl√™s) do dataset IMDb como "Positivas" ou "Negativas".

O notebook implementa um pipeline completo de Processamento de Linguagem Natural (PLN), desde o carregamento eficiente dos dados at√© a capacidade de realizar predi√ß√µes em texto puro, tornando-o um excelente exemplo de uma solu√ß√£o de PLN de ponta a ponta.

## ‚ú® Principais Caracter√≠sticas

- **Carregamento Eficiente de Dados**: Utiliza a API `tf.keras.utils.text_dataset_from_directory` para carregar e rotular dados de texto diretamente da estrutura de pastas, otimizando a performance com `.cache()` e `.prefetch()`.
- **Pr√©-processamento Integrado**: A camada `TextVectorization` do Keras √© usada para padronizar, tokenizar e vetorizar o texto. Por estar integrada ao modelo, garante que o mesmo pr√©-processamento seja aplicado de forma consistente no treino, valida√ß√£o e infer√™ncia.
- **Arquitetura RNN Moderna**: Emprega uma camada **LSTM Bidirecional**, que permite ao modelo capturar o contexto do texto lendo a sequ√™ncia tanto da esquerda para a direita quanto da direita para a esquerda.
- **Preven√ß√£o de Overfitting**: Implementa **Dropout** e **Early Stopping** (`EarlyStopping`), uma t√©cnica crucial que monitora a perda na valida√ß√£o e interrompe o treinamento quando o modelo para de melhorar, restaurando os melhores pesos encontrados.
- **Visualiza√ß√£o Clara dos Resultados**: Gera gr√°ficos de acur√°cia e perda durante o treinamento para uma an√°lise visual do comportamento e da generaliza√ß√£o do modelo.

## üìä Dataset

O projeto utiliza o **Large Movie Review Dataset (ACL IMDb v1)**. Este √© um dataset cl√°ssico para classifica√ß√£o bin√°ria de sentimentos.

- **Conte√∫do**: 50.000 cr√≠ticas de filmes.
- **Estrutura**:
  - `25.000` cr√≠ticas para treino.
  - `25.000` cr√≠ticas para teste.
- **R√≥tulos**: As cr√≠ticas j√° est√£o divididas em pastas `pos` (positivo) e `neg` (negativo).
- **Download**: [Dispon√≠vel em Stanford AI Group](https://ai.stanford.edu/~amaas/data/sentiment/)

## üèóÔ∏è Arquitetura do Modelo

O modelo foi constru√≠do de forma sequencial utilizando a API Keras do TensorFlow:

1.  **Camada de Entrada (`Input`)**: Recebe strings de texto puro com `shape=(1,)`.
2.  **Camada de Pr√©-processamento (`TextVectorization`)**: Transforma o texto em sequ√™ncias de inteiros.
    - `max_tokens` (tamanho do vocabul√°rio): 10.000
    - `output_sequence_length` (comprimento da sequ√™ncia): 250
3.  **Camada de Embedding (`Embedding`)**: Converte os inteiros em vetores densos, aprendendo a representa√ß√£o sem√¢ntica das palavras.
    - `input_dim`: 10.000
    - `output_dim`: 16
4.  **Camada RNN (`Bidirectional(LSTM)`)**: Processa as sequ√™ncias de vetores para extrair features contextuais.
    - Unidades LSTM: 32
5.  **Camadas de Classifica√ß√£o (`Dense` + `Dropout`)**:
    - Uma camada `Dense` com 32 neur√¥nios e ativa√ß√£o `relu`.
    - Uma camada de `Dropout` com taxa de `0.7` para regulariza√ß√£o.
    - A camada de sa√≠da `Dense` com 1 neur√¥nio e ativa√ß√£o `sigmoid` para a classifica√ß√£o bin√°ria.

```
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 text_vectorization_2 (TextV  (None, 250)               0
 ectorization)

 embedding_layer (Embedding) (None, 250, 16)           160000

 bidirectional_2 (Bidirection  (None, 64)                12544
 al)

 dense_2 (Dense)             (None, 32)                2080

 dropout_1 (Dropout)         (None, 32)                0

 dense_3 (Dense)             (None, 1)                 33

=================================================================
Total params: 174657 (682.25 KB)
Trainable params: 174657 (682.25 KB)
Non-trainable params: 0 (0.00 B)
_________________________________________________________________
```

## ‚öôÔ∏è Instala√ß√£o e Execu√ß√£o

Para executar este projeto localmente, siga os passos abaixo:

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/seu-usuario/seu-repositorio.git
    cd seu-repositorio
    ```

2.  **Crie e ative um ambiente virtual (recomendado):**
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\activate
    # macOS/Linux
    source .venv/bin/activate
    ```

3.  **Instale as depend√™ncias:**
    ```bash
    pip install tensorflow matplotlib
    ```

4.  **Baixe e prepare o dataset:**
    - Baixe o arquivo `aclImdb_v1.tar.gz` do [link oficial](https://ai.stanford.edu/~amaas/data/sentiment/).
    - Crie uma pasta `data` na raiz do projeto.
    - Descompacte o conte√∫do do arquivo dentro da pasta `data`, de forma que a estrutura final seja `data/aclImdb/`.

5.  **Execute o Jupyter Notebook:**
    ```bash
    jupyter notebook notebook.ipynb
    ```

## üìà Resultados e Avalia√ß√£o

O modelo foi treinado por 7 √©pocas, interrompido pelo callback `EarlyStopping` que restaurou os pesos da 4¬™ √©poca, onde a perda na valida√ß√£o foi m√≠nima.

A acur√°cia final alcan√ßada no conjunto de teste (dados nunca vistos) foi de **84.07%**.

### Curvas de Aprendizagem

O gr√°fico abaixo mostra a evolu√ß√£o da acur√°cia e da perda nos conjuntos de treino e valida√ß√£o. A proximidade entre as curvas de treino e valida√ß√£o indica que o modelo generalizou bem, evitando overfitting significativo.
![Gr√°fico de Acur√°cia e Perda de Treino e Valida√ß√£o](gr√°fico.png)


### Teste com Novas Frases

O modelo final √© capaz de classificar novas cr√≠ticas com alta confian√ßa:

| Cr√≠tica                                                                                       | Predi√ß√£o  | Confian√ßa |
| --------------------------------------------------------------------------------------------- | --------- | :-------: |
| "This movie was absolutely fantastic, I really loved it and would recommend it to everyone!"  | Positivo  |  92.27%   |
| "It was a complete waste of my time. The plot was predictable and the acting was subpar."      | Negativo  |  98.73%   |
| "The movie was okay, not great but not terrible either. A bit forgettable."                   | Negativo  |  93.93%   |
| "The special effects were good, but the story was incredibly boring."                         | Negativo  |  94.37%   |

## üöÄ Poss√≠veis Melhorias

- **Embeddings Pr√©-treinadas**: Utilizar embeddings como GloVe ou Word2Vec para inicializar a camada de `Embedding`, o que pode melhorar a performance, especialmente com datasets menores.
- **Arquiteturas Alternativas**: Experimentar com camadas **GRU (Gated Recurrent Unit)** ou arquiteturas mais complexas como **Transformers (ex: BERT)** para capturar rela√ß√µes contextuais mais sofisticadas.
- **Ajuste de Hiperpar√¢metros**: Realizar uma busca otimizada (usando KerasTuner ou Optuna) para encontrar a melhor combina√ß√£o de hiperpar√¢metros (ex: dimens√£o do embedding, unidades LSTM, taxa de dropout).
- **Deploy**: Empacotar o modelo treinado e implant√°-lo como uma API REST usando Flask ou FastAPI para uso em aplica√ß√µes reais.