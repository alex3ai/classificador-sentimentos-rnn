{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b867bcf",
   "metadata": {},
   "source": [
    "**Carregamento e PreparaÃ§Ã£o dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6eeff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ba070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ A pasta 'unsup' nÃ£o existe mais. Tudo limpo.\n",
      "\n",
      "Carregando dados de TREINO...\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Carregando dados de VALIDAÃ‡ÃƒO...\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Carregando dados de TESTE...\n",
      "Found 25000 files belonging to 2 classes.\n",
      "\n",
      "âœ… Classes finais: ['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURAÃ‡ÃƒO ---\n",
    "dataset_dir = os.path.join('data', 'aclImdb')\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# Remove a pasta 'unsup' para evitar problemas!\n",
    "unsup_dir = os.path.join(train_dir, 'unsup')\n",
    "if os.path.exists(unsup_dir):\n",
    "    shutil.rmtree(unsup_dir)\n",
    "    print(f\"ğŸ§¹ Pasta intrusa removida: {unsup_dir}\")\n",
    "else:\n",
    "    print(\"âœ¨ A pasta 'unsup' nÃ£o existe mais. Tudo limpo.\")\n",
    "\n",
    "# --- 1. CARREGAR TREINO ---\n",
    "print(\"\\nCarregando dados de TREINO...\")\n",
    "train_dataset = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# --- 2. CARREGAR VALIDAÃ‡ÃƒO ---\n",
    "print(\"Carregando dados de VALIDAÃ‡ÃƒO...\")\n",
    "validation_dataset = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# --- 3. CARREGAR TESTE ---\n",
    "print(\"Carregando dados de TESTE...\")\n",
    "test_dataset = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# --- VERIFICAÃ‡ÃƒO FINAL ---\n",
    "print(\"\\nâœ… Classes finais:\", train_dataset.class_names)\n",
    "# deve aparecer apenas: ['neg', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c249ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das classes: ['neg', 'pos']\n",
      "Pipeline de dados otimizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Salvando o nome das classes\n",
    "class_names = train_dataset.class_names\n",
    "print(f\"Nomes das classes: {class_names}\") # Deve imprimir ['neg', 'pos']\n",
    "\n",
    "# Aplicando otimizaÃ£o\n",
    "# A partir daqui, train_dataset muda de tipo e fica mais rÃ¡pido\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline de dados otimizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594211cf",
   "metadata": {},
   "source": [
    "**PrÃ©-processamento do Texto**\n",
    "\n",
    "O coraÃ§Ã£o do prÃ©-processamento serÃ¡ a camada TextVectorization. Ela executa trÃªs tarefas essenciais:\n",
    "\n",
    "1- PadronizaÃ§Ã£o: Limpa o texto (remove pontuaÃ§Ã£o, converte para minÃºsculas, etc.).\n",
    "\n",
    "2- TokenizaÃ§Ã£o: Divide o texto em palavras individuais (tokens).\n",
    "\n",
    "3- VetorizaÃ§Ã£o: Converte cada token em um nÃºmero inteiro.\n",
    "\n",
    "Esta camada serÃ¡ treinada com nossos dados e depois integrada diretamente ao modelo, garantindo que o prÃ©-processamento seja consistente durante o treino, avaliaÃ§Ã£o e inferÃªncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48968c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemplo de VetorizaÃ§Ã£o ---\n",
      "CrÃ­tica Original: \"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
      "RÃ³tulo: neg\n",
      "\n",
      "CrÃ­tica Vetorizada: [   1    7    4  194   18 2941   12  256  127   51  384   71  167  257\n",
      "   70]\n",
      "Palavras correspondentes: [np.str_('that'), np.str_('br'), np.str_('was'), np.str_('as'), np.str_('for'), np.str_('with'), np.str_('movie'), np.str_('but')]\n"
     ]
    }
   ],
   "source": [
    "# 1. Definir parÃ¢metros para a vetorizaÃ§Ã£o\n",
    "VOCAB_SIZE = 10000  # Manter as 10.000 palavras mais frequentes\n",
    "SEQUENCE_LENGTH = 250 # Padronizar todas as crÃ­ticas para terem 250 palavras\n",
    "\n",
    "# 2. Criar a camada de vetorizaÃ§Ã£o de texto\n",
    "# A camada jÃ¡ lida com a conversÃ£o para minÃºsculas e remoÃ§Ã£o de pontuaÃ§Ã£o por padrÃ£o\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# 3. Treinar a camada de vetorizaÃ§Ã£o nos dados de treino\n",
    "# Ã‰ crucial usar apenas os dados de treino para construir o vocabulÃ¡rio,\n",
    "# evitando vazamento de dados (data leakage) do conjunto de teste.\n",
    "# Mapeamos o dataset para extrair apenas o texto, descartando os rÃ³tulos.\n",
    "train_text = train_dataset.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "# --- VerificaÃ§Ã£o do Processo ---\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "# Exemplo de como um lote de texto Ã© convertido em nÃºmeros\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"--- Exemplo de VetorizaÃ§Ã£o ---\")\n",
    "print(\"CrÃ­tica Original:\", first_review.numpy().decode('utf-8'))\n",
    "# Usar a variÃ¡vel 'class_names' (salva antes de aplicar cache/prefetch) em vez de acessar attribute no dataset\n",
    "print(\"RÃ³tulo:\", class_names[int(first_label.numpy())])\n",
    "print(\"\\nCrÃ­tica Vetorizada:\", vectorize_layer(tf.constant([first_review.numpy()])).numpy()[0, :15])\n",
    "print(\"Palavras correspondentes:\", vectorize_layer.get_vocabulary()[12:20]) # Exemplo de vocabulÃ¡rio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2aeac2",
   "metadata": {},
   "source": [
    "**ConstruÃ§Ã£o do Modelo RNN (com LSTM)**\n",
    "\n",
    "Com o prÃ©-processamento definido, podemos construir a arquitetura da nossa rede neural.\n",
    "\n",
    "1- Camada de Embedding: Transforma os nÃºmeros inteiros (tokens) em vetores densos de tamanho fixo (embedding_dim). Isso permite que o modelo aprenda o significado e as relaÃ§Ãµes entre as palavras.\n",
    "\n",
    "2- Camada Bidirecional (LSTM): O nÃºcleo da nossa RNN. A LSTM (Long Short-Term Memory) Ã© uma variante de RNN eficaz para capturar dependÃªncias de longo prazo no texto. Usar Bidirectional permite que a rede processe a sequÃªncia de texto tanto da esquerda para a direita quanto da direita para a esquerda, capturando um contexto mais rico.\n",
    "\n",
    "3- Camadas Densas: As camadas finais que atuam como um classificador sobre as features extraÃ­das pela LSTM. A Ãºltima camada usa a ativaÃ§Ã£o sigmoid, pois a saÃ­da Ã© binÃ¡ria (Positivo/Negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e85ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimensÃ£o do Embedding (representaÃ§Ã£o vetorial de cada palavra)\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # 1. A camada de entrada que espera strings de texto\n",
    "    layers.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    # 2. A camada de prÃ©-processamento que definimos\n",
    "    vectorize_layer,\n",
    "    \n",
    "    # 3. Camada de Embedding: Mapeia o vocabulÃ¡rio para vetores densos\n",
    "    layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    \n",
    "    # 4. Camada LSTM Bidirecional para aprender com as sequÃªncias\n",
    "    # O uso de Bidirectional Ã© uma prÃ¡tica comum e eficaz em NLP\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(16)), # Adicionando uma segunda camada LSTM para mais profundidade\n",
    "    \n",
    "    # 5. Camadas de classificaÃ§Ã£o final\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5), # Dropout para regularizaÃ§Ã£o e combate ao overfitting\n",
    "    layers.Dense(1, activation='sigmoid') # SaÃ­da binÃ¡ria (0 ou 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841dfdba",
   "metadata": {},
   "source": [
    "**CompilaÃ§Ã£o e Treinamento do Modelo**\n",
    "\n",
    "Iremos configurar o otimizador, a funÃ§Ã£o de perda e as mÃ©tricas para monitorar o treinamento.\n",
    "\n",
    "*loss='binary_crossentropy': Ideal para problemas de classificaÃ§Ã£o binÃ¡ria.\n",
    "\n",
    "*optimizer='adam': Um otimizador robusto e amplamente utilizado.\n",
    "\n",
    "*metrics=['accuracy']: Para monitorar a acurÃ¡cia durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d677a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_vectorization_1            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,368</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_vectorization_1            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTextVectorization\u001b[0m)             â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m16\u001b[0m)        â”‚       \u001b[38;5;34m160,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚        \u001b[38;5;34m12,544\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m10,368\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m1,056\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,001</span> (718.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m184,001\u001b[0m (718.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,001</span> (718.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m184,001\u001b[0m (718.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Visualizar a arquitetura do modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc90f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o treinamento com Early Stopping...\n",
      "Epoch 1/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 172ms/step - accuracy: 0.9933 - loss: 0.0317 - val_accuracy: 0.8524 - val_loss: 0.7774\n",
      "Epoch 2/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 182ms/step - accuracy: 0.9937 - loss: 0.0279 - val_accuracy: 0.8532 - val_loss: 0.8144\n",
      "Epoch 3/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 191ms/step - accuracy: 0.9948 - loss: 0.0253 - val_accuracy: 0.8490 - val_loss: 0.8477\n",
      "Epoch 4/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 191ms/step - accuracy: 0.9962 - loss: 0.0204 - val_accuracy: 0.8456 - val_loss: 0.9570\n",
      "Treinamento concluÃ­do.\n"
     ]
    }
   ],
   "source": [
    "# Treinamento: alimentamos o modelo com os dados de treino e validaÃ§Ã£o.\n",
    "print(\"Iniciando o treinamento com Early Stopping...\")\n",
    "\n",
    "# 1. Definir o Early Stopping\n",
    "# patience=3: O treinamento para se a val_loss nÃ£o diminuir por 3 Ã©pocas consecutivas.\n",
    "# restore_best_weights=True: Restaura os pesos do modelo da Ã©poca onde a val_loss foi a menor.\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Definimos o EPOCHS para um nÃºmero alto, confiando no callback para parar no ponto certo\n",
    "EPOCHS = 100 \n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    # A CORREÃ‡ÃƒO ESTÃ AQUI: Passando o callback para o Keras\n",
    "    callbacks=[early_stopping_callback] \n",
    ")\n",
    "\n",
    "print(\"Treinamento concluÃ­do.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
