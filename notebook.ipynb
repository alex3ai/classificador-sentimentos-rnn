{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b867bcf",
   "metadata": {},
   "source": [
    "**Carregamento e Prepara√ß√£o dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eeff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ba070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® A pasta 'unsup' n√£o existe mais. Tudo limpo.\n",
      "\n",
      "Carregando dados de TREINO...\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Carregando dados de VALIDA√á√ÉO...\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Carregando dados de TESTE...\n",
      "Found 25000 files belonging to 2 classes.\n",
      "\n",
      "‚úÖ Classes finais: ['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURA√á√ÉO ---\n",
    "dataset_dir = os.path.join('data', 'aclImdb')\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# Remove a pasta 'unsup' para evitar problemas!\n",
    "unsup_dir = os.path.join(train_dir, 'unsup')\n",
    "if os.path.exists(unsup_dir):\n",
    "    shutil.rmtree(unsup_dir)\n",
    "    print(f\"üßπ Pasta intrusa removida: {unsup_dir}\")\n",
    "else:\n",
    "    print(\"‚ú® A pasta 'unsup' n√£o existe mais. Tudo limpo.\")\n",
    "\n",
    "# --- 1. CARREGAR TREINO ---\n",
    "print(\"\\nCarregando dados de TREINO...\")\n",
    "train_dataset = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# --- 2. CARREGAR VALIDA√á√ÉO ---\n",
    "print(\"Carregando dados de VALIDA√á√ÉO...\")\n",
    "validation_dataset = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# --- 3. CARREGAR TESTE ---\n",
    "print(\"Carregando dados de TESTE...\")\n",
    "test_dataset = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# --- VERIFICA√á√ÉO FINAL ---\n",
    "print(\"\\n‚úÖ Classes finais:\", train_dataset.class_names)\n",
    "# deve aparecer apenas: ['neg', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c249ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das classes: ['neg', 'pos']\n",
      "Pipeline de dados otimizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Salvando o nome das classes\n",
    "class_names = train_dataset.class_names\n",
    "print(f\"Nomes das classes: {class_names}\") # Deve imprimir ['neg', 'pos']\n",
    "\n",
    "# Aplicando otimiza√£o\n",
    "# A partir daqui, train_dataset muda de tipo e fica mais r√°pido\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline de dados otimizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594211cf",
   "metadata": {},
   "source": [
    "**Pr√©-processamento do Texto**\n",
    "\n",
    "O cora√ß√£o do pr√©-processamento ser√° a camada TextVectorization. Ela executa tr√™s tarefas essenciais:\n",
    "\n",
    "1- Padroniza√ß√£o: Limpa o texto (remove pontua√ß√£o, converte para min√∫sculas, etc.).\n",
    "\n",
    "2- Tokeniza√ß√£o: Divide o texto em palavras individuais (tokens).\n",
    "\n",
    "3- Vetoriza√ß√£o: Converte cada token em um n√∫mero inteiro.\n",
    "\n",
    "Esta camada ser√° treinada com nossos dados e depois integrada diretamente ao modelo, garantindo que o pr√©-processamento seja consistente durante o treino, avalia√ß√£o e infer√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48968c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemplo de Vetoriza√ß√£o ---\n",
      "Cr√≠tica Original: \"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
      "R√≥tulo: neg\n",
      "\n",
      "Cr√≠tica Vetorizada: [   1    7    4  194   18 2941   12  256  127   51  384   71  167  257\n",
      "   70]\n",
      "Palavras correspondentes: [np.str_('that'), np.str_('br'), np.str_('was'), np.str_('as'), np.str_('for'), np.str_('with'), np.str_('movie'), np.str_('but')]\n"
     ]
    }
   ],
   "source": [
    "# 1. Definir par√¢metros para a vetoriza√ß√£o\n",
    "VOCAB_SIZE = 10000  # Manter as 10.000 palavras mais frequentes\n",
    "SEQUENCE_LENGTH = 250 # Padronizar todas as cr√≠ticas para terem 250 palavras\n",
    "\n",
    "# 2. Criar a camada de vetoriza√ß√£o de texto\n",
    "# A camada j√° lida com a convers√£o para min√∫sculas e remo√ß√£o de pontua√ß√£o por padr√£o\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# 3. Treinar a camada de vetoriza√ß√£o nos dados de treino\n",
    "# √â crucial usar apenas os dados de treino para construir o vocabul√°rio,\n",
    "# evitando vazamento de dados (data leakage) do conjunto de teste.\n",
    "# Mapeamos o dataset para extrair apenas o texto, descartando os r√≥tulos.\n",
    "train_text = train_dataset.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "# --- Verifica√ß√£o do Processo ---\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "# Exemplo de como um lote de texto √© convertido em n√∫meros\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"--- Exemplo de Vetoriza√ß√£o ---\")\n",
    "print(\"Cr√≠tica Original:\", first_review.numpy().decode('utf-8'))\n",
    "# Usar a vari√°vel 'class_names' (salva antes de aplicar cache/prefetch) em vez de acessar attribute no dataset\n",
    "print(\"R√≥tulo:\", class_names[int(first_label.numpy())])\n",
    "print(\"\\nCr√≠tica Vetorizada:\", vectorize_layer(tf.constant([first_review.numpy()])).numpy()[0, :15])\n",
    "print(\"Palavras correspondentes:\", vectorize_layer.get_vocabulary()[12:20]) # Exemplo de vocabul√°rio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2aeac2",
   "metadata": {},
   "source": [
    "**Constru√ß√£o do Modelo RNN (com LSTM)**\n",
    "\n",
    "Com o pr√©-processamento definido, podemos construir a arquitetura da nossa rede neural.\n",
    "\n",
    "1- Camada de Embedding: Transforma os n√∫meros inteiros (tokens) em vetores densos de tamanho fixo (embedding_dim). Isso permite que o modelo aprenda o significado e as rela√ß√µes entre as palavras.\n",
    "\n",
    "2- Camada Bidirecional (LSTM): O n√∫cleo da nossa RNN. A LSTM (Long Short-Term Memory) √© uma variante de RNN eficaz para capturar depend√™ncias de longo prazo no texto. Usar Bidirectional permite que a rede processe a sequ√™ncia de texto tanto da esquerda para a direita quanto da direita para a esquerda, capturando um contexto mais rico.\n",
    "\n",
    "3- Camadas Densas: As camadas finais que atuam como um classificador sobre as features extra√≠das pela LSTM. A √∫ltima camada usa a ativa√ß√£o sigmoid, pois a sa√≠da √© bin√°ria (Positivo/Negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e85ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimens√£o do Embedding (representa√ß√£o vetorial de cada palavra)\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # 1. A camada de entrada que espera strings de texto\n",
    "    layers.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    # 2. A camada de pr√©-processamento que definimos\n",
    "    vectorize_layer,\n",
    "    \n",
    "    # 3. Camada de Embedding: Mapeia o vocabul√°rio para vetores densos\n",
    "    layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    \n",
    "    # 4. Camada LSTM Bidirecional para aprender com as sequ√™ncias\n",
    "    # O uso de Bidirectional √© uma pr√°tica comum e eficaz em NLP\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(16)), # Adicionando uma segunda camada LSTM para mais profundidade\n",
    "    \n",
    "    # 5. Camadas de classifica√ß√£o final\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5), # Dropout para regulariza√ß√£o e combate ao overfitting\n",
    "    layers.Dense(1, activation='sigmoid') # Sa√≠da bin√°ria (0 ou 1)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
